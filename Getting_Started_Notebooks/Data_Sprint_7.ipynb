{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Data_Sprint_7.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDisalNlaYQs"
      },
      "source": [
        "![dphi banner](https://dphi-courses.s3.ap-south-1.amazonaws.com/Datathons/dphi_banner.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zq0W0R_SWun_"
      },
      "source": [
        "# **Getting Started Code For [Data Sprint #7]( https://dphi.tech/challenges/datathon) on DPhi**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Xmeez26YdC2"
      },
      "source": [
        "#### **Author: Manish KC**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCfCO8Z90f0c"
      },
      "source": [
        "# Loading Libraries\n",
        "All Python capabilities are not loaded to our working environment by default (even they are already installed in your system). So, we import each and every library that we want to use.\n",
        "\n",
        "In data science, numpy and pandas are most commonly used libraries. Numpy is required for calculations like means, medians, square roots, etc. Pandas is used for data processin and data frames. We chose alias names for our libraries for the sake of our convenience (numpy --> np and pandas --> pd).\n",
        "\n",
        "Note: You can import all the libraries that you think will be required or can import it as you go along. \n",
        "\n",
        "Here we are importing two libraries - numpy and pandas\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdgJJlH40_bQ"
      },
      "source": [
        "import numpy as np        # Fundamental package for linear algebra and multidimensional arrays\n",
        "import pandas as pd       # Data analysis and manipultion tool"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1NzvInd1kx6"
      },
      "source": [
        "# Loading Dataset\n",
        "Pandas module is used for reading files. We have our data in '.csv' format. We will use 'read_csv()' function for loading the data. \n",
        "\n",
        "You can learn more about pandas [here](https://dphi.tech/learn/introduction-to-pandas)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5inGNMiR1hKx"
      },
      "source": [
        "# In read_csv() function, we have passed the location to where the files are located in the dphi official github page.\n",
        "bank_marketing_data  = pd.read_csv(\"https://raw.githubusercontent.com/dphi-official/Datasets/master/bank_marketing_data/training_set_label.csv\" )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ndV57522Czi"
      },
      "source": [
        "## What do you need to do now?\n",
        "*  Perform EDA and Data Visualization üëÄ to understand the data. Learn more about EDA [here](https://dphi.tech/learn/introduction-to-exploratory-data-analysis). Learn more about data visualization [here](https://dphi.tech/learn/introduction-to-data-visualization)\n",
        "*  Clean the data if required (like removing or filling missing values, treat outliers, etc.). Learn more about handling missing values [here](https://youtu.be/EaGbS7eWSs0)\n",
        "*  Perform Data Preprocessing if you feel it's required. Learn one hot encoding [here](https://youtu.be/9yl6-HEY7_s)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJZTl-Hn6SqG"
      },
      "source": [
        "# Separating Input Features and Output Features\n",
        "Before building any machine learning model, we always separate the input variables and output variables. Input variables are those quantities whose values are changed naturally in an experiment, whereas output variable is the one whose values are dependent on the input variables. So, input variables are also known as independent variables as its values are not dependent on any other quantity, and output variable/s are also known as dependent variables as its values are dependent on other variable i.e. input variables. Like here in this data, we want to predict if the client would subscribe to the product or not (i.e. y), so the y (this is a variable in the dataset) is our target variable i.e. y and remaining features are input variable.\n",
        "\n",
        "By convention input variables are represented with 'X' and output variables are represented with 'y'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bG7oHnzB1_NJ"
      },
      "source": [
        "# Input/independent variables\n",
        "X = bank_marketing_data.drop('y', axis = 1)   # her we are droping the y feature as this is the target and 'X' is input features, the changes are not \n",
        "                                              # made inplace as we have not used 'inplace = True'\n",
        "\n",
        "y = bank_marketing_data['y']             # Output/Dependent variable"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydj91s_E64Uu"
      },
      "source": [
        "# Splitting the data into Train and Validation Set\n",
        "We want to check the performance of the model that we built. For this purpose, we always split (both input and output data) the given data into training set which will be used to train the model, and test set which will be used to check how accurately the model is predicting outcomes.\n",
        "\n",
        "For this purpose we have a class called 'train_test_split' in the 'sklearn.model_selection' module.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7EGP5w360BD"
      },
      "source": [
        "# import train_test_split\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dj5_kJ2Y7FnS"
      },
      "source": [
        "# split the data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X,y,test_size=0.3, random_state = 42)\n",
        "\n",
        "# X_train: independent/input feature data for training the model\n",
        "# y_train: dependent/output feature data for training the model\n",
        "# X_test: independent/input feature data for testing the model; will be used to predict the output values\n",
        "# y_test: original dependent/output values of X_test; We will compare this values with our predicted values to check the performance of our built model.\n",
        " \n",
        "# test_size = 0.30: 30% of the data will go for test set and 70% of the data will go for train set\n",
        "# random_state = 42: this will fix the split i.e. there will be same split for each time you run the co"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsxciXev7Q_i"
      },
      "source": [
        "# Building Model\n",
        "Now we are finally ready, and we can train the model.\n",
        "\n",
        "There are tons of Machine Learning models like Linear Regression, Random Forest, Decision Tree, etc. to say you some. However here we are using Random Forest Regressor (again, using the sklearn library).\n",
        "\n",
        "Then we would feed the model both with the data (X_train) and the answers for that data (y_train)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ay4pnyq87LO8"
      },
      "source": [
        "# Importing RandomForestClassifier from sklearn.ensemble\n",
        "# We will be further discussing about why Random Forest is in ensemble module of sklearn library\n",
        "from sklearn.ensemble import RandomForestClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sE6vHN6t763C"
      },
      "source": [
        "rfc = RandomForestClassifier()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e23pEgaC8G8l"
      },
      "source": [
        "### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YmXvx3B7-FH"
      },
      "source": [
        "rfc.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKO513vA8Vhu"
      },
      "source": [
        "# Validate The Model\n",
        "Wonderü§î how well your model learned! Lets check it.\n",
        "\n",
        "### Predict on the validation data (X_val)\n",
        "Now we predict using our trained model on the validation set we created i.e. X_val and evaluate our model on unforeseen data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0d5YdS-W8RD1"
      },
      "source": [
        "pred = rfc.predict(X_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYD_9rcY9xkl"
      },
      "source": [
        "## Model Evaluation\n",
        "Evaluating performance of the machine learning model that we have built is an essential part of any machine learning project. Performance of our model is done using some evaluation metrics.\n",
        "\n",
        "There are so many evaluation metrics to use for regression problem, naming some - Accuracy Score, F1 Score, Precision, Recall etc. However, **F1 Score** is the metric for this challenge. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ce4XV0GS-u_r"
      },
      "source": [
        "# import mean squared error from sklearn.metric\n",
        "from sklearn.metrics import f1_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBI7JwY1-40F"
      },
      "source": [
        "print('F1 Score is: ', np.sqrt(f1_score(y_val, pred))) \n",
        "\n",
        "# y_val is the original target value of the validation set (X_val)\n",
        "# pred is the predicted target value of the validation set"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-b1C_HVx_lKV"
      },
      "source": [
        "# Predict The Output For Testing Dataset üòÖ\n",
        "We have trained our model, evaluated it and now finally we will predict the output/target for the testing data (i.e. testing_set_label.csv) given in 'Data' section of the problem page.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BlJRzftAjTO"
      },
      "source": [
        "## Load Test Set\n",
        "Load the test data on which final submission is to be made."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_LHXg8ZAilM"
      },
      "source": [
        "test_data = pd.read_csv('https://raw.githubusercontent.com/dphi-official/Datasets/master/bank_marketing_data/testing_set_label.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSlXRaJRA2K9"
      },
      "source": [
        "**Note:** \n",
        "*  Use the same techniques to deal with missing values as done with the training dataset.   \n",
        "\n",
        "*  **Don't remove any observation/record from the test dataset otherwise you will get wrong answer. The number of items in your prediction should be same as the number of records are present in the test dataset**.\n",
        "\n",
        "*  Use the same techniques to preprocess the data as done with training dataset.\n",
        "\n",
        "***Why do we need to do the same procedure of filling missing values, data cleaning and data preprocessing on the new test data as it was done for the training and validation data?***\n",
        "\n",
        "**Ans:** Because our model has been trained on certain format of data and if we don't provide the testing data of the similar format, the model will give erroneous predictions and the rmse of the model will increase. Also, if the model was build on 'n' number of features, while predicting on new test data you should always give the same number of features to the model. In this case if you provide different number of features while predicting the output, your ML model will throw a ValueError saying something like 'number of features given x; expecting n'. Not confident about these statements? Well, as a data scientist you should always perform some experiment and observe the results.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1VWI-oiCp3t"
      },
      "source": [
        "## Make Prediction on Test Dataset\n",
        "Time to make submission!!!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3Fsfy9iA1N7"
      },
      "source": [
        "target = rfc.predict(test_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrNaxq9KEp0n"
      },
      "source": [
        "#### Note: **Follow the submission guidelines given in 'How To Submit' Section.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iH2U3r7CC99s"
      },
      "source": [
        "## How to save prediciton results locally via jupyter notebook?\n",
        "If you are working on Jupyter notebook, execute below block of codes. A file named 'prediction_results.csv' will be created in your current working directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyT2vAGIDjnF"
      },
      "source": [
        "res = pd.DataFrame(target) #target is nothing but the final predictions of your model on input features of your new unseen test data\n",
        "res.index = test_data.index # its important for comparison. Here \"test_new\" is your new test dataset\n",
        "res.columns = [\"prediction\"]\n",
        "res.to_csv(\"submission.csv\")      # the csv file will be saved locally on the same location where this notebook is located."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98_QXo0TD_0G"
      },
      "source": [
        "# **OR**, \n",
        "**if you are working on Google Colab then use the below set of code to save prediction results locally**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfrE2CjoDwje"
      },
      "source": [
        "## How to save prediction results locally via colab notebook?\n",
        "If you are working on Google Colab Notebook, execute below block of codes. A file named 'prediction_results' will be downloaded in your system."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JgcOUpjKDzzI"
      },
      "source": [
        "# To create Dataframe of predicted value with particular respective index\n",
        "res = pd.DataFrame(target) # target are nothing but the final predictions of your model on input features of your new unseen test data\n",
        "res.index = test_data.index # its important for comparison. Here \"test_new\" is your new test dataset\n",
        "res.columns = [\"prediction\"]\n",
        "\n",
        "# To download the csv file locally\n",
        "from google.colab import files\n",
        "res.to_csv('submission.csv')         \n",
        "files.download('submission.csv')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VghEzJ8zE4kM"
      },
      "source": [
        "## **Well Done! üëç**\n",
        "You are all set to make a submission. Let's head to the [solve page](https://dphi.tech/practice/challenge/39#submission) to make the submission."
      ]
    }
  ]
}